package com.hello.spark.java.common;

import com.hello.spark.java.entity.Iris;
import org.apache.spark.Partition;
import org.apache.spark.Partitioner;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.Optional;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import scala.collection.Iterator;
import scala.io.Source;

import java.io.*;
import java.net.URL;
import java.util.ArrayList;
import java.util.List;

public class SparkCommon {

    private static String appName;
    private static String master;

    static {
        appName = "HelloSpark";
        master = "local[*]";
    }

    public static void main(String[] args) throws IOException {

        // demo();

        // getIrisDataSet();

        String filePath = "dataset/iris.data";
        getIrisDataSetFromResourcesDirectory(filePath);


    }

    private static void demo() {
        File file = new File("/dataset/iris.data");
        System.out.println("file.isFile : " + file.isFile()); // file.isFile : false
        System.out.println("file.exists : " + file.exists()); // file.exists : false

        System.out.println("----------------------------------------------------------");

        file = new File("./dataset/iris.data");
        System.out.println("file.isFile : " + file.isFile()); // file.isFile : false
        System.out.println("file.exists : " + file.exists()); // file.exists : false

        System.out.println("----------------------------------------------------------");

        file = new File("dataset/iris.data");
        System.out.println("file.isFile : " + file.isFile()); // file.isFile : false
        System.out.println("file.exists : " + file.exists()); // file.exists : false


        System.out.println("----------------------------------------------------------");

        // file = new File("/Users/hiahia/Downloads/iris.data"); // mac
        file = new File("C:\\Users\\calm\\Downloads\\iris.data"); // windows

        System.out.println("file.isFile : " + file.isFile()); // file.isFile : true
        System.out.println("file.exists : " + file.exists()); // file.exists : true

        System.out.println();
        System.out.println("||||||||||||||||||||||||||||||||||||||||||||||||||||||||||");
        System.out.println("||||||||||||||||||||||||||||||||||||||||||||||||||||||||||");
        System.out.println();
    }

    /**
     * åˆå§‹åŒ– SparkConf
     *
     * @return SparkConf
     */
    private static SparkConf initSparkConf() {
        SparkConf sparkConf = new SparkConf().setMaster(master).setAppName(appName);

//		sparkConf.set("spark.driver.allowMultipleContexts", "true");
//		sparkConf.set("spark.eventLog.enabled", "true");
//		sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
//		sparkConf.set("spark.hadoop.validateOutputSpecs", "false");
//		sparkConf.set("hive.mapred.supports.subdirectories", "true");
//		sparkConf.set("mapreduce.input.fileinputformat.input.dir.recursive", "true");

        return sparkConf;
    }

    /**
     * é€šè¿‡ SparkConf åˆå§‹åŒ– JavaSparkContext
     *
     * @return JavaSparkContext
     */
    private static JavaSparkContext initJavaSparkContextBySparkConf(SparkConf sparkConf) {
        JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf);
        return javaSparkContext;
    }

    /**
     * åˆå§‹åŒ– SparkSession
     *
     * @return SparkSession
     */
    private static SparkSession initSparkSession() {
        SparkConf sparkConf = initSparkConf();
        SparkSession sparkSession = SparkSession.builder().appName(appName).config(sparkConf).getOrCreate();
        return sparkSession;
    }

    /**
     * åˆå§‹åŒ– SparkSession å¹¶å¯åŠ¨ hive æ”¯æŒ
     *
     * @return SparkSession
     */
    private static SparkSession initSparkSessionAndEnableHiveSupport() {
        SparkConf sparkConf = initSparkConf();
        SparkSession sparkSession = SparkSession.builder().appName(appName).config(sparkConf).enableHiveSupport().getOrCreate();
        return sparkSession;
    }

    /**
     * é€šè¿‡ SparkSession åˆå§‹åŒ– JavaSparkContext
     *
     * @return JavaSparkContext
     */
    private static JavaSparkContext initJavaSparkContextBySparkSession(SparkSession sparkSession) {
        JavaSparkContext javaSparkContext = new JavaSparkContext(sparkSession.sparkContext());
        return javaSparkContext;
    }

    /**
     * å…³é—­ SparkSession
     *
     */
    private static void stopSparkSession(SparkSession sparkSession) {
        sparkSession.stop();
    }

    /**
     * å…³é—­ JavaSparkContext
     *
     */
    private static void stopSparkSession(JavaSparkContext javaSparkContext) {
        javaSparkContext.stop();
    }






    /**
     * è¯»å–é¸¢å°¾èŠ±æ•°æ®é›†
     *
     * æµ‹è¯•æ•°æ®:http://archive.ics.uci.edu/ml/machine-learning-databases/iris/
     *
     * ç‰¹å¾
     *  è¯¥æ•°æ®é›†æµ‹é‡äº†æ‰€æœ‰150ä¸ªæ ·æœ¬çš„4ä¸ªç‰¹å¾ï¼Œåˆ†åˆ«æ˜¯ï¼š
     *
     *      1ã€sepal lengthï¼ˆèŠ±è¼é•¿åº¦ï¼‰
     *      2ã€sepal widthï¼ˆèŠ±è¼å®½åº¦ï¼‰
     *      3ã€petal lengthï¼ˆèŠ±ç“£é•¿åº¦ï¼‰
     *      4ã€petal widthï¼ˆèŠ±ç“£å®½åº¦ï¼‰
     * ä»¥ä¸Šå››ä¸ªç‰¹å¾çš„å•ä½éƒ½æ˜¯å˜ç±³ï¼ˆcmï¼‰ã€‚
     *
     * é€šå¸¸ä½¿ç”¨ğ‘šè¡¨ç¤ºæ ·æœ¬é‡çš„å¤§å°ï¼Œğ‘›è¡¨ç¤ºæ¯ä¸ªæ ·æœ¬æ‰€å…·æœ‰çš„ç‰¹å¾æ•°ã€‚å› æ­¤åœ¨è¯¥æ•°æ®é›†ä¸­ï¼Œğ‘š=150,ğ‘›=4
     *
     *
     */
    private static void getIrisDataSet() {
        getIrisDataSetFromResourcesDirectory();
    }

    /**
     * è¯»å–æ–‡ä»¶é—®é¢˜
     *
     *  Sparkè¯»å–resourcesç›®å½•ä¸­æ–‡ä»¶
     *  https://blog.csdn.net/LINBE_blazers/article/details/104012275
     */
    private static void getIrisDataSetFromResourcesDirectory() {
        SparkSession sparkSession = initSparkSession();

        // Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/hiahia/develop/workspace/IntelliJ-IDEA/github/hello/dataset/iris.data;
        // Dataset<Row> irisDataSet = sparkSession.read().text("./dataset/iris.data");

        // Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/hiahia/develop/workspace/IntelliJ-IDEA/github/hello/dataset/iris.data;
        // Dataset<Row> irisDataSet = sparkSession.read().text("dataset/iris.data");

        // Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: file:/dataset/iris.data;
        // Dataset<Row> irisDataSet = sparkSession.read().text("/dataset/iris.data");

        // InputStream resourceAsStream = SparkCommon.class.getClassLoader().getResourceAsStream("./dataset/iris.data");

    }

    /**
     * java spark list è½¬ä¸º RDD è½¬ä¸º dataset å†™å…¥è¡¨ä¸­
     * https://www.cnblogs.com/Allen-rg/p/11365013.html
     * @throws IOException
     */
    private static void getIrisDataSetFromResourcesDirectory(String filePath) throws IOException {

        InputStream in = SparkCommon.class.getClassLoader().getResourceAsStream("dataset/iris.data");

        InputStreamReader isr = new InputStreamReader(in);
        BufferedReader br = new BufferedReader(isr);
        String line;
        List<String> lines = new ArrayList<>();
        while ((line = br.readLine()) != null) {
            System.out.println(line);
            lines.add(line);
        }


        SparkSession sparkSession = initSparkSession();
        JavaSparkContext javaSparkContext = initJavaSparkContextBySparkSession(sparkSession);

        JavaRDD<String> linesRDD = javaSparkContext.parallelize(lines);

        /** - begin - filter */
        JavaRDD<String> filter;

        JavaRDD<String> filter_1 = linesRDD.filter(new Function<String, Boolean>() {
            @Override
            public Boolean call(String line) throws Exception {
                if (line != null && line.length() > 0 && line.trim().length() > 0) {
                    return true;
                }
                return false;
            }
        });

        JavaRDD<String> filter_2 = linesRDD.filter((String line_1) -> {
            if (line_1 != null && line_1.length() > 0 && line_1.trim().length() > 0) {
                return true;
            }
            return false;
        });

        filter = filter_2;

        /** - end - filter */

        /** - begin - map */



        /** - end - map */

        /** - begin - get DataFrame */

        // Dataset<Row> dataFrame = reflectTransform(sparkSession, filter);
        Dataset<Row> dataFrame = dataFrame = dynamicTransform(sparkSession, filter);

        /** - end - get DataFrame */

        System.out.println("- printSchema ------------------------------------");

        dataFrame.printSchema();

        System.out.println("- show --------------------------------------------");

        dataFrame.show(); // åªæ˜¾ç¤ºå‰20æ¡è®°å½•ã€‚

        System.out.println("- show --------------------------------------------");

        dataFrame.show(10); // æ˜¾ç¤ºnumRowsæ¡

        System.out.println("- show --------------------------------------------");

        dataFrame.show(false); // æ˜¯å¦æœ€å¤šåªæ˜¾ç¤º20ä¸ªå­—ç¬¦ï¼Œé»˜è®¤ä¸ºtrueã€‚

        System.out.println("- show --------------------------------------------");

        dataFrame.show(10, false); // ç»¼åˆå‰é¢çš„æ˜¾ç¤ºè®°å½•æ¡æ•°ï¼Œä»¥åŠå¯¹è¿‡é•¿å­—ç¬¦ä¸²çš„æ˜¾ç¤ºæ ¼å¼ã€‚


    }

    /**
     * é€šè¿‡Javaåå°„è½¬æ¢
     * @param sparkSession
     *          sparkSession
     * @return Dataset<Row>
     */
    private static Dataset<Row> reflectTransform(SparkSession sparkSession, JavaRDD<String> linesRDD) {
        JavaRDD<Iris> irisRDD;

        JavaRDD<Iris> irisRDD_1 = linesRDD.map(new Function<String, Iris>() {

            @Override
            public Iris call(String line) throws Exception {
                String[] split = line.split(",");
                Iris iris = new Iris();

                iris.setSepalLengthCm(Float.parseFloat(split[0].trim()));
                iris.setSepalWidthCm(Float.parseFloat(split[1].trim()));
                iris.setPetalLengthCm(Float.parseFloat(split[2].trim()));
                iris.setPetalWidthCm(Float.parseFloat(split[3].trim()));

                iris.setSpecies(split[4].trim());

                return iris;
            }
        });

        JavaRDD<Iris> irisRDD_2 = linesRDD.map(line -> {
            String[] split = line.split(",");
            Iris iris = new Iris();

            iris.setSepalLengthCm(Float.parseFloat(split[0].trim()));
            iris.setSepalWidthCm(Float.parseFloat(split[1].trim()));
            iris.setPetalLengthCm(Float.parseFloat(split[2].trim()));
            iris.setPetalWidthCm(Float.parseFloat(split[3].trim()));

            iris.setSpecies(split[4].trim());

            return iris;
        });

        irisRDD = irisRDD_2;

        Dataset<Row> dataFrame = sparkSession.createDataFrame(irisRDD, Iris.class);

        return dataFrame;
    }

    /**
     * åŠ¨æ€è½¬æ¢
     * @param sparkSession
     *          sparkSession
     * @return Dataset<Row>
     */
    private static Dataset<Row> dynamicTransform(SparkSession sparkSession, JavaRDD<String> linesRDD) {

        JavaRDD<Row> rowRDD;

        JavaRDD<Row> rowRDD_1 = linesRDD.map(new Function<String, Row>() {

            @Override
            public Row call(String line) throws Exception {
                String[] split = line.split(",");

                float sepalLengthCm = Float.parseFloat(split[0].trim());
                float sepalWidthCm = Float.parseFloat(split[1].trim());
                float petalLengthCm = Float.parseFloat(split[2].trim());
                float petalWidthCm = Float.parseFloat(split[3].trim());

                String species = split[4].trim();

                Row row = RowFactory.create(sepalLengthCm, sepalWidthCm, petalLengthCm, petalWidthCm, species);

                return row;
            }
        });

        JavaRDD<Row> rowRDD_2 = linesRDD.map((String line) -> {
            String[] split = line.split(",");

            float sepalLengthCm = Float.parseFloat(split[0].trim());
            float sepalWidthCm = Float.parseFloat(split[1].trim());
            float petalLengthCm = Float.parseFloat(split[2].trim());
            float petalWidthCm = Float.parseFloat(split[3].trim());

            String species = split[4].trim();

            Row row = RowFactory.create(sepalLengthCm, sepalWidthCm, petalLengthCm, petalWidthCm, species);

            return row;
        });

        rowRDD = rowRDD_2;

        List<StructField> fields = new ArrayList<>();
        StructField field = null;

        field = DataTypes.createStructField("sepalLength", DataTypes.FloatType, true);
        fields.add(field);

        field = DataTypes.createStructField("sepalWidth", DataTypes.FloatType, true);
        fields.add(field);

        field = DataTypes.createStructField("petalLength", DataTypes.FloatType, true);
        fields.add(field);

        field = DataTypes.createStructField("petalWidth", DataTypes.FloatType, true);
        fields.add(field);

        field = DataTypes.createStructField("species", DataTypes.StringType, true);
        fields.add(field);

        StructType schema = DataTypes.createStructType(fields);

        Dataset<Row> dataFrame = sparkSession.createDataFrame(rowRDD, schema);

        return dataFrame;

    }



}
