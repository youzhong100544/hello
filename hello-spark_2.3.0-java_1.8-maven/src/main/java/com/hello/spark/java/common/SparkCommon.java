package com.hello.spark.java.common;

import org.apache.spark.Partition;
import org.apache.spark.Partitioner;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.Optional;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import scala.collection.Iterator;
import scala.io.Source;

import java.io.*;
import java.net.URL;
import java.util.ArrayList;
import java.util.List;

public class SparkCommon {

    private static String appName;
    private static String master;

    static {
        appName = "HelloSpark";
        master = "local[*]";
    }

    public static void main(String[] args) throws IOException {

        // demo();

        // getIrisDataSet();

        String filePath = "dataset/iris.data";
        getIrisDataSetFromResourcesDirectory(filePath);


    }

    private static void demo() {
        File file = new File("/dataset/iris.data");
        System.out.println("file.isFile : " + file.isFile()); // file.isFile : false
        System.out.println("file.exists : " + file.exists()); // file.exists : false

        System.out.println("----------------------------------------------------------");

        file = new File("./dataset/iris.data");
        System.out.println("file.isFile : " + file.isFile()); // file.isFile : false
        System.out.println("file.exists : " + file.exists()); // file.exists : false

        System.out.println("----------------------------------------------------------");

        file = new File("dataset/iris.data");
        System.out.println("file.isFile : " + file.isFile()); // file.isFile : false
        System.out.println("file.exists : " + file.exists()); // file.exists : false


        System.out.println("----------------------------------------------------------");

        // file = new File("/Users/hiahia/Downloads/iris.data"); // mac
        file = new File("C:\\Users\\calm\\Downloads\\iris.data"); // windows

        System.out.println("file.isFile : " + file.isFile()); // file.isFile : true
        System.out.println("file.exists : " + file.exists()); // file.exists : true

        System.out.println();
        System.out.println("||||||||||||||||||||||||||||||||||||||||||||||||||||||||||");
        System.out.println("||||||||||||||||||||||||||||||||||||||||||||||||||||||||||");
        System.out.println();
    }

    /**
     * åˆå§‹åŒ– SparkConf
     *
     * @return SparkConf
     */
    private static SparkConf initSparkConf() {
        SparkConf sparkConf = new SparkConf().setMaster(master).setAppName(appName);

//		sparkConf.set("spark.driver.allowMultipleContexts", "true");
//		sparkConf.set("spark.eventLog.enabled", "true");
//		sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
//		sparkConf.set("spark.hadoop.validateOutputSpecs", "false");
//		sparkConf.set("hive.mapred.supports.subdirectories", "true");
//		sparkConf.set("mapreduce.input.fileinputformat.input.dir.recursive", "true");

        return sparkConf;
    }

    /**
     * é€šè¿‡ SparkConf åˆå§‹åŒ– JavaSparkContext
     *
     * @return JavaSparkContext
     */
    private static JavaSparkContext initJavaSparkContextBySparkConf() {
        SparkConf sparkConf = initSparkConf();
        JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf);
        return javaSparkContext;
    }

    /**
     * åˆå§‹åŒ– SparkSession
     *
     * @return SparkSession
     */
    private static SparkSession initSparkSession() {
        SparkConf sparkConf = initSparkConf();
        SparkSession sparkSession = SparkSession.builder().appName(appName).config(sparkConf).getOrCreate();
        return sparkSession;
    }

    /**
     * åˆå§‹åŒ– SparkSession å¹¶å¯åŠ¨ hive æ”¯æŒ
     *
     * @return SparkSession
     */
    private static SparkSession initSparkSessionAndEnableHiveSupport() {
        SparkConf sparkConf = initSparkConf();
        SparkSession sparkSession = SparkSession.builder().appName(appName).config(sparkConf).enableHiveSupport().getOrCreate();
        return sparkSession;
    }

    /**
     * é€šè¿‡ SparkSession åˆå§‹åŒ– JavaSparkContext
     *
     * @return JavaSparkContext
     */
    private static JavaSparkContext initJavaSparkContextBySparkSession() {
        SparkSession sparkSession = initSparkSession();
        JavaSparkContext javaSparkContext = new JavaSparkContext(sparkSession.sparkContext());
        return javaSparkContext;
    }

    /**
     * å…³é—­ SparkSession
     *
     */
    private static void stopSparkSession(SparkSession sparkSession) {
        sparkSession.stop();
    }

    /**
     * å…³é—­ JavaSparkContext
     *
     */
    private static void stopSparkSession(JavaSparkContext javaSparkContext) {
        javaSparkContext.stop();
    }






    /**
     * è¯»å–é¸¢å°¾èŠ±æ•°æ®é›†
     *
     * æµ‹è¯•æ•°æ®:http://archive.ics.uci.edu/ml/machine-learning-databases/iris/
     *
     * ç‰¹å¾
     *  è¯¥æ•°æ®é›†æµ‹é‡äº†æ‰€æœ‰150ä¸ªæ ·æœ¬çš„4ä¸ªç‰¹å¾ï¼Œåˆ†åˆ«æ˜¯ï¼š
     *
     *      1ã€sepal lengthï¼ˆèŠ±è¼é•¿åº¦ï¼‰
     *      2ã€sepal widthï¼ˆèŠ±è¼å®½åº¦ï¼‰
     *      3ã€petal lengthï¼ˆèŠ±ç“£é•¿åº¦ï¼‰
     *      4ã€petal widthï¼ˆèŠ±ç“£å®½åº¦ï¼‰
     * ä»¥ä¸Šå››ä¸ªç‰¹å¾çš„å•ä½éƒ½æ˜¯å˜ç±³ï¼ˆcmï¼‰ã€‚
     *
     * é€šå¸¸ä½¿ç”¨ğ‘šè¡¨ç¤ºæ ·æœ¬é‡çš„å¤§å°ï¼Œğ‘›è¡¨ç¤ºæ¯ä¸ªæ ·æœ¬æ‰€å…·æœ‰çš„ç‰¹å¾æ•°ã€‚å› æ­¤åœ¨è¯¥æ•°æ®é›†ä¸­ï¼Œğ‘š=150,ğ‘›=4
     *
     *
     * @return JavaSparkContext
     */
    private static void getIrisDataSet() {
        getIrisDataSetFromResourcesDirectory();
    }

    /**
     * è¯»å–æ–‡ä»¶é—®é¢˜
     *
     *  Sparkè¯»å–resourcesç›®å½•ä¸­æ–‡ä»¶
     *  https://blog.csdn.net/LINBE_blazers/article/details/104012275
     */
    private static void getIrisDataSetFromResourcesDirectory() {
        SparkSession sparkSession = initSparkSession();

        // Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/hiahia/develop/workspace/IntelliJ-IDEA/github/hello/dataset/iris.data;
        // Dataset<Row> irisDataSet = sparkSession.read().text("./dataset/iris.data");

        // Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/hiahia/develop/workspace/IntelliJ-IDEA/github/hello/dataset/iris.data;
        // Dataset<Row> irisDataSet = sparkSession.read().text("dataset/iris.data");

        // Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: file:/dataset/iris.data;
        // Dataset<Row> irisDataSet = sparkSession.read().text("/dataset/iris.data");

        // InputStream resourceAsStream = SparkCommon.class.getClassLoader().getResourceAsStream("./dataset/iris.data");

    }

    /**
     * java spark list è½¬ä¸º RDD è½¬ä¸º dataset å†™å…¥è¡¨ä¸­
     * https://www.cnblogs.com/Allen-rg/p/11365013.html
     * @throws IOException
     */
    private static void getIrisDataSetFromResourcesDirectory(String filePath) throws IOException {

        InputStream in = SparkCommon.class.getClassLoader().getResourceAsStream("dataset/iris.data");

        InputStreamReader isr = new InputStreamReader (in);
        BufferedReader br = new BufferedReader(isr);
        String line;
        List<String> lines = new ArrayList<>();
        while ((line = br.readLine()) != null) {
            System.out.println(line);
            lines.add(line);
        }


        JavaSparkContext javaSparkContext = initJavaSparkContextBySparkSession();

        JavaRDD<String> linesRDD = javaSparkContext.parallelize(lines);

        int numPartitions = linesRDD.getNumPartitions();
        System.out.println("getNumPartitions:" + numPartitions);

        linesRDD.map(new Function<String, Object>() {

        });

        linesRDD


    }



}
